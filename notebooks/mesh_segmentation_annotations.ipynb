{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class2rank(name: dict) -> list[int]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return [ord(c) - ord('a') + 1 for c in name]\n",
    "\n",
    "\n",
    "def compute_rank_stats(path_annotations: Path | str, path_metadata: Path | str, key: str) -> dict:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    metadata = json.load(open(path_metadata))\n",
    "    annotations = {}\n",
    "    for filename in glob.glob(str(path_annotations / 'annotations/worker-response/iteration-1/*/*.json')):\n",
    "        modelid = Path(filename).parent.stem\n",
    "        annotations[int(modelid)] = json.load(open(filename))['answers']\n",
    "    assert len(metadata) == len(annotations)\n",
    "\n",
    "    method2ranks = {}\n",
    "    for i in range(len(metadata)):\n",
    "        ranks = []\n",
    "        for ans in annotations[i]:\n",
    "            ranks.append(class2rank(ans['answerContent']['crowd-classifier']['label']))\n",
    "        ranks_average = []\n",
    "        for j in range(len(ranks[0])):\n",
    "            ranks_average.append(sum([r[j] for r in ranks]) / len(ranks))\n",
    "        for method, index in metadata[i].items():\n",
    "            method2ranks.setdefault(method, []).append(ranks_average[index])\n",
    "    method2rank_stats = {\n",
    "        method: {\n",
    "            'avg': sum(ranks) / len(ranks),\n",
    "            'std': np.std(ranks),\n",
    "        } \n",
    "        for method, ranks in method2ranks.items()\n",
    "    }\n",
    "    return method2rank_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'combined': {'avg': 1.176, 'std': 0.2712145522152772}, 'shape_diameter_function': {'avg': 1.824, 'std': 0.2712145522152772}}\n"
     ]
    }
   ],
   "source": [
    "print(compute_rank_stats(\n",
    "    Path('/home/ubuntu/meshseg/tests/mesh-segmentation-samesh-v-sdf-annotations'), \n",
    "    Path('/home/ubuntu/meshseg/tests/mesh_segmentation-samesh-v-sdf/metadata.json'),\n",
    "    key='mesh-segmentation-high-quality-metadata'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'combined': {'avg': 2.3253333333333335, 'std': 0.5912908665698202}, 'matte': {'avg': 2.5866666666666664, 'std': 0.6946142014736589}, 'norm': {'avg': 2.5653333333333332, 'std': 0.577233247675688}, 'sdf': {'avg': 2.5226666666666664, 'std': 0.7503907130436931}}\n"
     ]
    }
   ],
   "source": [
    "print(compute_rank_stats(\n",
    "    Path('/home/ubuntu/meshseg/tests/mesh-segmentation-samesh-modalities-annotations'),\n",
    "    Path('/home/ubuntu/meshseg/tests/mesh_segmentation-samesh-modalities/metadata.json'),\n",
    "    key='mesh-segmentation-ablation-high-quality-metadata'\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meshseg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
